#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Jun 22 12:10:34 2018

@author: Anton Buyskikh
@brief: Regulirized linear regression. Bias vs variance.
...
"""

#%% libraries

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import timeit
import scipy.io
from sklearn.preprocessing import PolynomialFeatures

#%% functions

def h(X,theta):
    # linear regression hypothesis
    return X.dot(theta)



def getCostReg(theta,X,y,lam_par):
    m,n=X.shape
    J=(-y*np.log(h(X,theta))-(1-y)*(np.log(1-h(X,theta)))).mean()
    J+=lam_par*(theta[1:]**2).sum()/2/m
    return J



def getGradReg(theta,X,y,lam_par):
    m,n=X.shape
    grad=(h(X,theta)-y).dot(X)/m
    grad[1:]+=lam_par/m*theta[1:]
    return grad

#%% PART I
# load data

data=scipy.io.loadmat('data/ex5data1.mat')
data.keys()

#%% extract data

# load data
X_train=data.get('X')
y_train=data.get('y')
X_test =data.get('Xtest')
y_test =data.get('ytest')
X_cv   =data.get('Xval')
y_cv   =data.get('yval')

# add polynomial features, default 1
poly=PolynomialFeatures(1)
X_train=poly.fit_transform(X_train)
X_test =poly.fit_transform(X_test )
X_cv   =poly.fit_transform(X_cv   )

# print shapes
print('X_train:',X_train.shape)
print('y_train:',y_train.shape)
print('X_test:' ,X_test.shape)
print('y_test:' ,y_test.shape)
print('X_cv:'   ,X_cv.shape)
print('y_cv:'   ,y_cv.shape)

#%% visualize data

fig,ax=plt.subplots()
ax.plot(X_train[:,1],y_train,'xk')
ax.plot(X_test[:,1] ,y_test ,'ob')
ax.plot(X_cv[:,1]   ,y_cv   ,'.r')
ax.set_xlabel('Change in water level (x)')
ax.set_ylabel('Water flowing out of the dam (y)')
ax.legend(['train','test','cv']) 
fig.show()

#%% polynomilap features





















