#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Jun 22 12:10:34 2018

@author: Anton Buyskikh
@brief: Regulirized linear regression. Bias vs variance.
...
"""

#%% libraries

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import timeit
import scipy.io

from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

#%% functions

def h(X,theta):
    # linear regression hypothesis
    return X.dot(theta)



def getCostReg(theta,X,y,lam_par):
    m,n=X.shape
    J=((h(X,theta)-y)**2).mean()/2+lam_par*(theta[1:]**2).sum()/2/m
    return J



def getGradReg(theta,X,y,lam_par):
    m=X.shape[0]
    gradJ=(h(X,theta)-y).dot(X)/m
    gradJ[1:]+=lam_par/m*theta[1:]
    return gradJ



def trainLinearReg(theta_init,X,y,lam_par,disp_opt=True):
    res=scipy.optimize.minimize(getCostReg,\
                                theta_init,\
                                args=(X,y,lam_par),\
                                method='Newton-CG',\
                                tol=1e-5,\
                                jac=getGradReg,\
                                options={'maxiter':1000,'disp':disp_opt})    
    return res



def learningCurve(X_train,y_train,X_cv,y_cv,lam_par):
    # total length of the training set
    m=y_train.size
    error_train_mean=np.zeros(m)
    error_train_std =np.zeros(m)
    error_cv_mean   =np.zeros(m)
    error_cv_std    =np.zeros(m)
    
    # One chooses niter sample sets of the length i and obrain error for
    # the training set as well as for the cross-validation set.
    # Then one averages over all iterations.
    niter=200;
    
    for i in range(m):
        tmp_train=np.zeros(niter)
        tmp_cv   =np.zeros(niter)
        
        for iter in range(niter):
            sample=np.random.randint(0,m,i+1)
            theta_init=np.random.uniform(low=-1,high=1,size=X_train.shape[1])
            res=trainLinearReg(theta_init,X_train[sample,:],y_train[sample],lam_par,disp_opt=False)
            tmp_train[iter]=getCostReg(res.x,X_train[:i+1,:],y_train[:i+1],0.0)
            tmp_cv   [iter]=getCostReg(res.x,X_cv           ,y_cv         ,0.0)
        
        error_train_mean[i]=tmp_train.mean()
        error_train_std[i] =tmp_train.std()
        error_cv_mean[i]   =tmp_cv.mean()
        error_cv_std[i]    =tmp_cv.std()
    
    return (error_train_mean,error_train_std,error_cv_mean,error_cv_std)

#%% PART I
# load data

data=scipy.io.loadmat('data/ex5data1.mat')
data.keys()

#%% extract data

# load data
X_train=data.get('X')
y_train=data.get('y').ravel()
X_test =data.get('Xtest')
y_test =data.get('ytest').ravel()
X_cv   =data.get('Xval')
y_cv   =data.get('yval').ravel()

# add polynomial features, default 1
poly=PolynomialFeatures(1)
X_train=poly.fit_transform(X_train)
X_test =poly.fit_transform(X_test )
X_cv   =poly.fit_transform(X_cv   )

# print shapes
print('X_train:',X_train.shape)
print('y_train:',y_train.shape)
print('X_test:' ,X_test.shape)
print('y_test:' ,y_test.shape)
print('X_cv:'   ,X_cv.shape)
print('y_cv:'   ,y_cv.shape)

#%% visualize data

fig,ax=plt.subplots()
ax.plot(X_train[:,1],y_train,'xk',label='train')
ax.plot(X_test[:,1] ,y_test ,'ob',label='test')
ax.plot(X_cv[:,1]   ,y_cv   ,'sr',label='cross-validation')
ax.set_xlabel('Change in water level (x)')
ax.set_ylabel('Water flowing out of the dam (y)')
ax.legend() 
fig.show()

#%% define initial theta and find its cost function with its gradient

lam_par=0.0
theta_init=np.ones(X_train.shape[1])
J=getCostReg(theta_init,X_train,y_train,0.0)
gradJ=getGradReg(theta_init,X_train,y_train,0.0)
print('Cost functiona at the initial point: ',J)
print('Gradient at the initial point: ',gradJ)

#%% find optimal theta using the training set

res=trainLinearReg(theta_init,X_train,y_train,lam_par)
print('theta optimal (manual): ',res.x)
print('Cost funstion minimum (manual): ',res.fun)

#%% comparison with LinearRegression in Scikit-learn

regr=LinearRegression(fit_intercept=False)
regr.fit(X_train,y_train)
print('theta optimal (Scikit-learn): ',regr.coef_)
print('Cost funstion minimum (Scikit-learn): ',getCostReg(regr.coef_,X_train,y_train,lam_par))
# one should see the same results

#%% plot the result with only linear features

fig,ax=plt.subplots()
ax.plot(X_train[:,1],y_train,'xk',label='train')
ax.plot(np.linspace(-50,40),(res.x[0]+(res.x[1]*np.linspace(-50,40))),label='linear fit')
ax.set_xlabel('Change in water level (x)')
ax.set_ylabel('Water flowing out of the dam (y)')
ax.legend() 
fig.show()

# this is clearly an underfitting model, hence we should observe an evidence
# for high bias

#%% investigate bias-variance crosover of the model

error_train_mean,error_train_std,error_cv_mean,error_cv_std=\
    learningCurve(X_train,y_train,X_cv,y_cv,0.0)

#%% plot the learning curve

fig,ax=plt.subplots()
ax.errorbar(range(1,len(y_train)+1),error_train_mean,error_train_std,label='Training error')
ax.errorbar(range(1,len(y_train)+1),error_cv_mean   ,error_cv_std   ,label='Validation error')
ax.set_title('Learning curve for linear regression')
ax.set_xlabel('Number of training examples')
ax.set_ylabel('Error')
ax.legend()
ax.set_ylim(ymin=0)
fig.show()

# As it was expected we see high error as the size of the samles increases.
# Hence the model has the high bias (underfitting).

#%%
























